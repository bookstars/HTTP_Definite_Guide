# 웹 로봇

-   `웹 로봇`은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램입니다.

-   웹 로봇은 그 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등 다양한 이름으로 불립니다.

## 9.1 크롤러와 크롤링

-   웹 크롤러는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹 페이지를 가져오는 재귀적 반복을 통해 웹을 순회하는 로봇입니다.

### 9.1.1 어디에서 시작하는가: '루트 집합(root set)'

-   **루트 집합**은 크롤러가 방문을 시작하는 URL들의 집합입니다.

      <img width="361" alt="2019-03-02 3 49 27" src="https://user-images.githubusercontent.com/44794974/53678539-cae96380-3d03-11e9-8889-9dd8b7f755e2.png">

-   위에 예시를 설명

    -   A를 시작으로 크롤링을 시작한다면 연결된 링크를 통해 J와 K까지 갈 수는 있지만, G와 N으로 가는 링크는 없습니다.

    -   그러므로 해당 링크들을 모두 크롤링 하기 위해서는 **루트 집합**을 A,G,S가 있어야 합니다.

### 9.1.2 링크 추출과 상대 링크 정상화

-   크롤러가 HTML 문서를 탐색할때 새로운 링크를 찾게 되면, 크롤링할 페이지 목록에 추가하게 됩니다.

-   이 때, 링크가 상대 링크로 되있을 경우 **절대 링크**로 변환할 필요가 있습니다.

### 9.1.3 순환 피하기

-   크롤링 할 때에는, 그들이 어디에 방문했는지 체크하여 같은 곳을 다시 방문하게 되는 **순환**을 피해야 합니다.

~~### 9.1.4 루프와 중복~~ 너무 뻔한 내용이라 따로 정리 하지 않겠습니다.

### 9.1.5 빵 부스러기의 흔적

-   수십억 개의 URL을 관리하기 하기에는 큰 메모리와 순환을 피하는 방법이 요구 됩니다. 이번에는 이를 관리 하기 위한 유용한 기법에 대해 살펴 보겠습니다.

    -   트리와 해시 테이블

    -   느슨한 존재 비트맵

        -   공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러들은 **존재 비트 배열(presence bit array)** 과 같은 느슨한 자료 구조를 사용합니다.
        -   각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 **'존재 비트(presence bit)'** 를 갖습니다.
        -   만약 URL에 방문했을때 존재 비트가 존재한다면, 크롤링 되었다고 간주 합니다.

    -   체크포인트

        -   크롤러가 갑자기 중단되었을 경우, 방문한 URL의 목록이 디스크에 저장되었는지 확인합니다.

    -   파티셔닝
        -   웹이 성장하면서, 한 대의 컴퓨터에서 하나의 크롤링이 완수하는 것은 불가능 해졌습니다.
        -   대규모 웹 크롤러는, 각각이 분리된 한 대의 컴퓨터인 크롤러가 동시에 다른 '한 부분'을 할당하여 책임을 지도록 해야 합니다.

### 9.1.6 별칭(alias)과 로봇 순환

-   URL이 별칭을 가질 수 있는 이상, 어떤 페이지를 이전에 방문 했었는지 말해주는 게 쉽지 않을 때도 있습니다.

      <img width="581" alt="2019-03-02 4 30 06" src="https://user-images.githubusercontent.com/44794974/53678886-772d4900-3d08-11e9-9894-447846d7eeab.png">

### 9.1.7 URL 정규화 하기

-   별칭과 같은 문제 때문에 URL들의 표준 형식을 **'정규화'** 하여 중복되는 리소스를 미리 제거하기 위해 노력합니다.

-   URL 정규화 방법

    1. 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가합니다.
    2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환합니다.
    3. #태그들을 제거합니다.

-   이 단계를 통해 위 별칭 예시의 a부터 c가지의 문제를 해결 할 수 있습니다.

-   하지만 각 웹 서버에 대한 지식 없이 크롤러가 d에서 f까지의 문제는 중복을 피할 수 없습니다.
    -   d 케이스: 웹 서버가 대소문자를 구분하는지 알아야 합니다.
    -   e 케이스: URL들이 같은 리소스를 가리키는지 알려면 이 디렉토리에 대한 웹 서버의 색인 페이지 설정을 알아야 합니다.
    -   f 케이스: URL의 IP 주소가 같은 물리적 컴퓨터를 참조한다는 것뿐 아니라, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지도 알아야 합니다.

### 9.1.8 파일 시스템 링크 순환

-   파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉토리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있습니다.

      <img width="182" alt="2019-03-03 11 18 22" src="https://user-images.githubusercontent.com/44794974/53690018-181e1180-3da6-11e9-86c5-f636cfa3ecbd.png">

-   위 그림의 동작을 본다면,

    1. GET http://www.foo.com/index.html
        - /index.html을 가져와서, subdir/index.html로 이어지는 링크를 발견합니다.
    2. GET http://www.foo.com/subdir/index.html
        - subdir.index.html을 가져왔지만 같은 index.html로 되돌아갑니다.
    3. GET http://www.foo.com/subdir/subdir/index.html
        - subdir/subdir/index.html을 가져옵니다.
    4. GET http://www.foo.com/subdir/subdir/subdir/index.html
        - subdir/subdir/subdir/index.html을 가져옵니다.

-   위 예제는, subdir/ 이 /로 링크되어 있기때문에 순한되고 있습니다.

~~### 9.1.9 동적 가상 웹 공간~~ -> 가상 URL을 만들어 크롤러를 교착 상태로 빠진다는 말을 장황하게 써두었습니다.

### 9.1.10 루프와 중복 피하기

-   정규화

    -   URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL을 회피합니다.(9.1.7 참고)

-   너비 우선 크롤링

    -   방문할 URL들을 깊이 우선 방식보다 너비 우선 방식을 사용한다면 순환의 영향을 최소화 할 수 있습니다.

-   스로틀링

    -   크롤러가 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한합니다.

-   URL 크기 제한

    -   URL의 길이가 길 경우(보통 1KB)는 크롤러가 크롤링을 거부하도록 합니다.
    -   하지만, 길이를 제한할 경우 가져오지 못하는 콘텐츠 들이 발생할 수 있어 적절한 에러 로그를 남길 수 있도록 해야 됩니다.

-   URL/사이트 블랙리스트

    -   로봇 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어 관리하고, 피할 수 있도록 합니다.

-   패턴 발견

    -   파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있습니다.
    -   ex, 위 순환 참조와 같이 /subdir/subdir/subdir/index.html
    -   그러므로, 이러한 주기가 2회 이상 반복될 경우, 패턴을 감지하여 순환을 방지합니다.

-   콘텐츠 지문(fingerprint)

    -   콘텐츠 지문을 사용하면, 페이지의 코넨츠에서 몇 바이트를 얻어내어 체크섬(checksum)을 계산합니다.

        -   체크섬(checksum): 페이지의 내용을 간락하게한 표현
            -   체크섬 함수: MD5와 같은 메시지 요약 함수

    -   만약 크롤러가 체크섬(checksum)이 같은 페이지게 가게 된다면 크롤링을 하지 않습니다.

## 9.2 로봇의 HTTP

-   크롤러 또한 HTTP 클라이언트와 같이 HTTP 명세의 규칙을 지켜야 합니다.
-   많은 크롤러는 요구사항이 적은 HTTP/1.0 요청을 보냅니다.

### 9.2.1 요청 헤더 식별하기

-   최소한의 HTTP를 지원하려고 함에도, 대부분은 약간의 신원 실별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송합니다.
-   크롤러들은 크롤러의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤드를 사이트에게 보내주는게 좋습니다.
-   다음은 크롤러들이 가지도록 권장되는 기본적인 신원 식별 헤더들입니다.

    -   User-Agent
        -   서버에게 요청을 만든 로봇의 이름을 말해줍니다.
    -   From
        -   로봇의 사용자/관리자의 이메일 주소를 제공합니다.
    -   Accept
        -   서버에게 어떤 미디어 타입을 보내도 되는지 말해 줍니다.
        -   이 헤드는 로봇이 관심 있는 유형의 콘텐츠만 받게 도움을 줍니다.
    -   Referer
        -   현재의 요청 URL을 포함한 문서의 URL을 제공합니다.

### 9.2.2 가상 호스팅

-   HTTP/1.1을 사용할 경우 Host 헤더를 사용할 것을 요구합니다.
-   만약 Host 헤더가 없을 경우, 두개의 사이트를 운영하는 웹 서버에서 요청과 다른 페이지를 보내어 문제가 일어날 수 있습니다.

    <img width="606" alt="2019-03-03 2 32 44" src="https://user-images.githubusercontent.com/44794974/53691434-4ceb9200-3dc1-11e9-8e59-864b9c80dd6a.png">

### 9.2.3 조건부 요청

-   크롤러들이 극악의 양을 요청을 시도한다는 것을 고려할 때, 검색하는 콘텐츠의 양을 최소화하는 것은 상당희 의미 있는 일입니다.
-   그 방법중에는, 시간이나 엔터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후에 업데이트 된것이 있는지 알아보는 HTTP 요청을 만드는 것입니다. (이는 7장 캐시의 재검사 방법과 유사합니다.)

### 9.2.4 응답 다루기

-   HTTP의 특정 몇몇 기능(조건부 요청 같은)을 사용하는 크롤러일 경우, 웹 탐색이나 서버와의 상호작용을 위해 여러 종류의 HTTP응답을 다룰 줄 알아야 합니다.

-   상태 코드

    -   크롤러들은 최소한 일반적인 상태코드(200 OK, 404 Not Found 등)를 알고 응답해야 합니다.

-   엔터티
    -   HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있습니다.
    -   메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보입니다.

### 9.2.5 User-Agent targeting

-   웹 관리자들은 많은 크롤러들이 그들의 사이트를 방문하게 될 것임을 명심하고, 클롤러들의 요청을 예상해야 합니다.

## 9.3 부적절하게 동작하는 로봇들

-   폭주하는 로봇

    -   만약 크롤러가 논리적인 에러나 순환에 빠져 있다면 웹 서버에 극심한 부하를 안겨줄 수 있습니다.

-   오래된 URL

    -   만약 URL을 최신으로 계속해서 업데이트를 하지 않는다면, 비어 있는 URL이거나 잘못된 컨텐츠 값을 가지고 있을 수 있습니다.
        <img width="606" alt="2019-03-03 2 32 44" src="https://user-images.githubusercontent.com/44794974/53691434-4ceb9200-3dc1-11e9-8e59-864b9c80dd6a.png">

-   길고 잘못된 URL

    -   순환이나 프로그래밍상의 오류로 인해 길고 의미 없는 URL이 요청 될 수 있습니다.

-   호기심이 지나친 로봇

    -   크롤러들이 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있습니다.

-   동적 게이트웨이 접근
    -   웹 사이트의 동적인 컨텐츠를 처리하기 위해 크롤러들이 계속해서 요청을 보내는건 웹 서버에 부담이 될 수 있습니다.

## 9.4 로봇 차단하기

-   1994년, 크롤러들이 그들에게 맞지 않는 장소에 들어오지 않도록 하고 웹 마스터에게 크롤러의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안되었습니다.
-   이 표준은 "Robots Exclusion Standard"라고 이름 지어졌지만, 크롤러의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 robots.txt라고 불립니다.

### robots.txt

-   어떤 웹 서버는 서브의 문서 루트에 **robots.txt**라고 이름 붙인 선택적인 파일을 제공할 수 있습니다.
-   만약 크롤러가 이 자발적인 표준에 따른다면, 리소스에 접근 전에 **robots.txt**파일에 접근할 것입니다.
-   그리고 **robots.txt**에서 웹 마스터가 정한 권한을 확인한 후 이에 맞는 동작을 시행할 것입니다.

### 9.4.1 로봇 차단 표준

-   로봇 차단 표준은 임시방편으로 마련된 표준입니다.

-   차단 표준에는 아래와 같이 세 가지 버전이 있지만, 2.0v은 복잡하여 대부분 1.0v을 따르고 있습니다.

    <img width="579" alt="2019-03-03 6 18 28" src="https://user-images.githubusercontent.com/44794974/53693226-cba3f780-3de0-11e9-9072-4b3ea8d58711.png">

### 9.4.2 웹 사이트와 robots.txt 파일들

-   크롤러가 웹사이트에 접근할때, robots.txt 파일이 존재한다면 반드시 그 파일을 가지고와 처리해야 합니다.
-   웹 마스터는 모든 콘텐츠에 대한 차단 규칙을 종합적으로 기술한 robots.txt파일을 생성할 책임이 있습니다.

#### robots.txt 가져오기

-   HTTP Get 메서드를 이용해 robots.txt 리소스를 가져옵니다.
-   서버는 robots.txt의 text/plain 본문을 반환합니다.
-   만약 서버가 404 Not Found HTTP 상태 코드로 응답한다면, 제한없이 요청을 할 수 있습니다.
-   크롤러는 관리자가 접근을 추척할 수 있도록 From이나 User-Agent헤더를 통해 신원 정보와 연락처를 넘겨야 합니다.

#### 응답 코드

-   크롤러는 robot.txt의 검색 결과에 따라 다르게 동작합니다.

    -   서버가 성공(HTTP 상태 코드 2xx)으로 응답하면 크롤러는 받드시 응답에서 콘텐츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려 할 때 그 규칙을 따라야 햡니다.

    -   만약 리소스가 존재하지 않는다고 서버가 응답하면(HTTP 상태 코드 404) 크롤러는 활성화된 차단 규칙이 존재하지 않는다고 가정하고 robot.txt 제약 없이 그 사이트에 접근할 수 있습니다.

    -   만약 서버가 접근 제한(HTTP 상태 코드 401 혹은 403)으로 응답한다면 로봇은 그 사이트로의 접근은 완전히 제한되어 있다고 가정해야 합니다.

    -   만약 요청 시도가 일시적으로 실패했다면(HTTP 상태 코드 503) 크롤러는 그 사이트의 리소스를 검색하는 것은 뒤로 미루어야 합니다.

    -   만약 서버 응답이 리다이렉션을 의미한다면(HTTP 상태 코드 3xx) 로봇은 리소스가 별견될 때까지 리다이렉트를 따라가야 합니다.

### 9.4.3 robots.txt 파일 포맷

-   robots.txt는 **빈 줄**, **주석 줄**, **규칙 줄** 세 가지 종류가 있습니다.
-   **규칙 줄**은 HTTP 헤더처럼 생겼고(<필드 값>: <값>) 패턴 매칭을 위해 사용 됩니다.

    ```JSON
        # 이 robots.txt 파일은 Slurp과 WebCrawler가 우리 사이트의 공개된
        # 영역을 크롤링하는 것을 허락한다. 그러나 다른 로봇은 안 된다...

        User-Agent: slurp
        User-Agent: webcrawler
        Disallow: /private

        User-Agent: *
        Disallow:
    ```

-   robots.txt의 줄들은 레코드로 구분됩니다. 각 레코드는 특정 크롤러들의 집합에 대한 차단 규칙의 집합을 기술 합니다.

-   각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝(end-fo-file)문자로 끝납니다.

#### Usre-Agent 줄

-   각 크롤러의 레코드는 하나 이상의 User-Agent 줄로 시작하며 형식은 다음과 같습니다.

    `User-Agent: <robot-name>`

    `User-Agent: *`

-   만약 크롤러가 자신의 이름에 대응하는 User-Agent 줄을 찾지 못하였고 와일드 카드를 사용한 "`User-Agent: *`" 줄도 찾지 못했다면, 대응하는 레코드가 없는 것이브로, 접근에는 어떤 제한도 없습니다.

#### Disallow와 Allow 줄들

-   Disallow와 Allow 줄은 로본 차단 레코드의 User-Agent 줄들 바로 다음에 옵니다.

-   이 줄들은 특정 크롤러에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술 합니다.

#### Disallow/Allow 접두 매칭(prefix matching)

-   Disallow/Allow를 매칭하는 방법은 다음과 같은 접두 매칭(prefix matching)이 사용됩니다.

    <img width="581" alt="스크린샷 2019-03-11 오후 10 57 45" src="https://user-images.githubusercontent.com/44794974/54130877-8f157300-4454-11e9-9fd4-dc7f5a5f4699.png">

### 9.4.4 그 외에 알아둘 점

-   robots.txt 파일은 명세가 발전함에 따라 User-Agent, Disallow, Allow 외의 다른 필드를 포함할 수 있습니다.

-   하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않습니다.

-   주석은 파일의 어디에서든 허용됩니다.

### 9.4.5 robots.txt의 캐싱과 만료

-   크롤러는 robots.txt를 캐시해두어 파일이 만료되기 전까지 사용하게 됩니다.

-   캐싱을 제어하기 우해 표준 HTTP 캐시 제어 메커니즘이 원 서버와 크롤러 양쪽 모두에 의해 사용됩니다.(로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 합니다.)

~~~9.4.6 크롤러 차단 펄 코드~~~ -> 라이브러리에 대한 설명으로 스킵하게 습니다.

### 9.4.7 HTML 크롤러 제어 META 태그

-   robots.txt 파일의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유한다는 것입니다.

-   HTML 페이지 저자는 로봇이 개별 페이지에 접근하는 것을 제한하는 좀 더 직접적인 방법을 갖고 있습니다.

-   HTML 문서에 직접 로봇 제어 태그를 추가할 수 있습니다.

    `<META NAME="ROBOTS" CONTENT=directive-list>`

#### 로봇 META 지시자

-   NOINDEX

    -   크롤러에게 이 페이지를 처리하지 말고 무시하라고 말해줍니다.

        `<META NAME="ROBOTS" CONTENT="NOINDEX">`

-   NOFOLLOW

    -   크롤러에게 이 페이지가 링크한 페이지를 크롤링하지 말라고 해줍니다.

        `<META NAME="ROBOTS" CONTENT="NOFOOLOW">`

-   INDEX

    -   크롤러에게 이 페이지의 콘텐츠를 인덱싱해도 된다고 말해줍니다.

-   FOLLOW

    -   크롤러에게 이 페이지가 링크한 페이지를 크롤링해도 된다고 말해줍니다.

-   NOARCHIVE

    -   크롤러에게 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안 된다고 말해줍니다.

-   ALL

    -   INDEX, FOLLOW와 같습니다.

-   NONE

    -   NOINDEX, NOFOLLOW와 같습니다.

-   크롤러 META 태그는 다른 모두 HTML META 태그와 마찬가지로 반드시 HTML 페이지의 HEAD 섹션에 나타나야 합니다.

```html
<html>
	<head>
		<meta name="robots" content="noindex, nofollow" />
		<title>...</title>
	</head>

	<body>
		...
	</body>
</html>
```

#### 검색엔진 META 태그

-   다음 표에서 제시한 것을 포함하여 다른 많은 META 태그들은 검색엔진 크롤러들에 대해 유용하게 사용할 수 있습니다.

    <img width="584" alt="스크린샷 2019-03-12 오전 12 05 18" src="https://user-images.githubusercontent.com/44794974/54134086-a0617e00-445a-11e9-93d0-222fed0221d1.png">

~~~9.5 로봇 에티켓~~ -> 크롤러의 가이드라인 이므로 스킵하겠습니다.

## 9.6 검색엔진

-   웹 클롤러가 주된 공급자인 검색엔진이 어떻게 동작하는지 간략하게 알아보겠습니다.

### 9.6.2 현대적인 검색엔진 아키텍처

-   오늘날 검색엔진들은 웹페이지들에 대해 '풀 텍스트 색인(full-text indexs)'이라고 하는 복잡한 로컬 데이터베이스를 생성합니다.

-   이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작합니다.

    <img width="553" alt="스크린샷 2019-03-12 오전 12 14 42" src="https://user-images.githubusercontent.com/44794974/54134734-d8b58c00-445b-11e9-9aec-09c05154c426.png">

### 9.6.3 풀 텍스트 색인

-   **풀 텍스트 색인**은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스입니다.

    <img width="339" alt="스크린샷 2019-03-12 오전 12 16 47" src="https://user-images.githubusercontent.com/44794974/54134914-27632600-445c-11e9-8d23-7c27bdefb535.png">

-   위 예제를 예를 들면,

    -   단어 'a'는 문서 A와 B에 들어있습니다.
    -   단어 'best'는 문서 A와 C에 들어있습니다.
    -   단어 'drill'는 문서 A와 B에 들어있습니다.
    -   단어 'the'는 세 문서 A,B,C 모두 들어있습니다.

### 9.6.4 질의 보내기

-   사용자는 HTML폼을 채워, HTTP GET 또는 POST 요청을 이용해서 게이트웨이로 보냅니다.

-   게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환 합니다.

    <img width="479" alt="스크린샷 2019-03-12 오전 12 28 52" src="https://user-images.githubusercontent.com/44794974/54135801-d5bb9b00-445d-11e9-9ca7-6ae8a5665194.png">

### 9.6.5 검색 결과를 정렬하고 보여주기

-   검색엔진은 문서들이 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타날 수 있도록 문서들 간의 순서를 알 필요가 있습니다.

-   이것은 관련도 랭킹(relevancy ranking)이라고 불리며, 검색 결과의 목록에 점수를 매기고 정렬하는 과정입니다.

-   이 과정을 더 잘 지원하기 위해, 많은 검색엔진이 웹을 크롤링하는 과정에서 수집된 통계 데이터를 실제로 사용합니다.

### 9.6.6 스푸핑

-   검색 결과에서 더 높은 쉬위를 차지하고자 하는 웹 마스터들의 노력은 검색 시스템과의 줄다리기를 만들어 냅니다.
    (ex, 수많은 키워드들을 나열한 가짜 페이지 만들기, 검색 엔진의 관련도 알고리즘을 더 잘 속일 수 있는, 특정 단어에 대한 가짜 페이지를 생성하는 게이트 웨이 애플리케이션 생성)